{"cells":[{"cell_type":"markdown","metadata":{},"source":["%%"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","import keras_nlp\n","import tensorflow as tf\n","import tensorflow_datasets as tfds\n","import tensorflow_text as tf_text\n","from tensorflow import keras\n","from tensorflow.lite.python import interpreter\n","import time"]},{"cell_type":"markdown","metadata":{},"source":["%%"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["gpt2_tokenizer = keras_nlp.models.GPT2Tokenizer.from_preset(\"gpt2_base_en\")\n","gpt2_preprocessor = keras_nlp.models.GPT2CausalLMPreprocessor.from_preset(\n","    \"gpt2_base_en\",\n","    sequence_length=256,\n","    add_end_token=True,\n",")\n","gpt2_lm = keras_nlp.models.GPT2CausalLM.from_preset(\"gpt2_base_en\", preprocessor=gpt2_preprocessor)"]},{"cell_type":"markdown","metadata":{},"source":["%%"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["output = gpt2_lm.generate(\"My trip to Yosemite was\", max_length=200)\n","print(\"\\nGPT-2 output:\")\n","print(output.numpy().decode(\"utf-8\"))"]},{"cell_type":"markdown","metadata":{},"source":["%%"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["output = gpt2_lm.generate(\"That Italian restaurant is\", max_length=200)\n","print(\"\\nGPT-2 output:\")\n","print(output.numpy().decode(\"utf-8\"))"]},{"cell_type":"markdown","metadata":{},"source":["%%"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["cnn_ds = tfds.load('cnn_dailymail', as_supervised=True)"]},{"cell_type":"markdown","metadata":{},"source":["%"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from nltk import tokenize\n","\n","import nltk\n","nltk.download('punkt')\n","\n","for article, highlights in cnn_ds['train']:\n","  combination = article + tf.constant(' TL;DR ') + tf.strings.regex_replace(highlights, \"\\n\", \" \")\n","  word_count=len(tokenize.word_tokenize(str(combination.numpy())))\n","  if word_count < 256:\n","    print(combination.numpy())\n","    print(article.numpy())\n","    print(highlights.numpy())\n","    break"]},{"cell_type":"markdown","metadata":{},"source":["%%"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import progressbar\n","\n","short_texts = []\n","total = len(cnn_ds['train'])\n","progressbar_update_freq = 1000\n","count = 0\n","\n","widgets = [' [',\n","         progressbar.Timer(format= 'elapsed time: %s'),\n","         '] ',\n","           progressbar.Bar('*'),' (',\n","           progressbar.ETA(), ') ',\n","          ]\n","bar = progressbar.ProgressBar(\n","    maxval=total // progressbar_update_freq + 2,\n","    widgets=widgets).start()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for article, highlights in cnn_ds['train']:\n","  combination = article + tf.constant(' TL;DR ') + tf.strings.regex_replace(highlights, \"\\n\", \" \")\n","  word_count = len(tokenize.word_tokenize(str(combination.numpy())))\n","  if word_count < 256:\n","    short_texts.append(combination)\n","  count += 1\n","  if count % progressbar_update_freq == 0:\n","    bar.update(count / progressbar_update_freq)"]},{"cell_type":"markdown","metadata":{},"source":["%%"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def save_texts(texts):\n","    np.savez('data/selected_texts.npz', texts)\n","    \n","def load_texts():\n","    restored_texts = list()\n","    with np.load('data/selected_texts.npz', allow_pickle=True) as data:\n","      for file in data.files:\n","        restored_texts.extend(data[file].tolist())\n","    return restored_texts"]},{"cell_type":"markdown","metadata":{},"source":["Save the list of short combinations of articles and summaries (sort of a checkpoint)."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["save_texts(short_texts)"]},{"cell_type":"markdown","metadata":{},"source":["%%"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# short_texts = load_texts()"]},{"cell_type":"markdown","metadata":{},"source":["%%"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tf_train_ds = tf.data.Dataset.from_tensor_slices(short_texts)\n","processed_ds = tf_train_ds.map(gpt2_preprocessor, tf.data.AUTOTUNE).batch(20).cache().prefetch(tf.data.AUTOTUNE)\n","part_of_ds = processed_ds.take(100)"]},{"cell_type":"markdown","metadata":{},"source":["%%"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["gpt2_lm.include_preprocessing = False\n","\n","num_epochs = 1\n","\n","lr = tf.keras.optimizers.schedules.PolynomialDecay(\n","    5e-5,\n","    decay_steps=part_of_ds.cardinality() * num_epochs,\n","    end_learning_rate=0.0,\n",")\n","\n","loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","\n","gpt2_lm.compile(\n","    optimizer=keras.optimizers.experimental.Adam(lr),\n","    loss=loss,\n","    weighted_metrics=[\"accuracy\"])\n","\n","gpt2_lm.fit(part_of_ds, epochs=num_epochs)"]},{"cell_type":"markdown","metadata":{},"source":["%%"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":2}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbfIM-rtxz0q"
      },
      "source": [
        "%%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9kN3h7UYxz0s",
        "outputId": "a1f43531-4b59-4f3b-e592-e6e71ed43bda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q git+https://github.com/keras-team/keras-nlp.git@google-io-2023 tensorflow-text==2.12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SIEWadhKxz0s"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import keras_nlp\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_text as tf_text\n",
        "from tensorflow import keras\n",
        "from tensorflow.lite.python import interpreter\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PkFrgp1xz0t"
      },
      "source": [
        "%%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1gGrymhOxz0t",
        "outputId": "d7270ae9-86c9-4686-f1cb-607e9fc06828",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul), but are not present in its tracked objects:   <tf.Variable 'token_embedding/embeddings:0' shape=(50257, 768) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n"
          ]
        }
      ],
      "source": [
        "gpt2_tokenizer = keras_nlp.models.GPT2Tokenizer.from_preset(\"gpt2_base_en\")\n",
        "gpt2_preprocessor = keras_nlp.models.GPT2CausalLMPreprocessor.from_preset(\n",
        "    \"gpt2_base_en\",\n",
        "    sequence_length=256,\n",
        "    add_end_token=True,\n",
        ")\n",
        "gpt2_lm = keras_nlp.models.GPT2CausalLM.from_preset(\"gpt2_base_en\", preprocessor=gpt2_preprocessor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTgAfB31xz0t"
      },
      "source": [
        "%%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUCSbh8bxz0t"
      },
      "outputs": [],
      "source": [
        "output = gpt2_lm.generate(\"My trip to Yosemite was\", max_length=200)\n",
        "print(\"\\nGPT-2 output:\")\n",
        "print(output.numpy().decode(\"utf-8\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ch-Y8JWqxz0t"
      },
      "source": [
        "%%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rK_AMcaPxz0u"
      },
      "outputs": [],
      "source": [
        "output = gpt2_lm.generate(\"That Italian restaurant is\", max_length=200)\n",
        "print(\"\\nGPT-2 output:\")\n",
        "print(output.numpy().decode(\"utf-8\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHEYMrJmxz0u"
      },
      "source": [
        "%%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSZmf18Fxz0u"
      },
      "outputs": [],
      "source": [
        "cnn_ds = tfds.load('cnn_dailymail', as_supervised=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrCnxSYTxz0u"
      },
      "source": [
        "%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ywXIS8ESxz0u",
        "outputId": "e0707857-2fc0-4e00-9aca-dd6c4a9630ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-a0a17ad2c085>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'punkt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marticle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhighlights\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcnn_ds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/downloader.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self, info_or_id, download_dir, quiet, force, prefix, halt_on_error, raise_on_error, print_error_to)\u001b[0m\n\u001b[1;32m    775\u001b[0m                 )\n\u001b[1;32m    776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mmsg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mincr_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo_or_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m                 \u001b[0;31m# Error messages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mErrorMessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/downloader.py\u001b[0m in \u001b[0;36mincr_download\u001b[0;34m(self, info_or_id, download_dir, force)\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0;31m# Handle Packages (delegate to a helper function).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_download_package\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_num_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/downloader.py\u001b[0m in \u001b[0;36m_download_package\u001b[0;34m(self, info, download_dir, force)\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munzip\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzipdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mStartUnzipMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mmsg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_unzip_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzipdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    736\u001b[0m                     \u001b[0;31m# Somewhat of a hack, but we need a proper package reference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m                     \u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpackage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/downloader.py\u001b[0m in \u001b[0;36m_unzip_iter\u001b[0;34m(filename, root, verbose)\u001b[0m\n\u001b[1;32m   2252\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2254\u001b[0;31m     \u001b[0mzf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2256\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36mextractall\u001b[0;34m(self, path, members, pwd)\u001b[0m\n\u001b[1;32m   1645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1646\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mzipinfo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmembers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1647\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extract_member\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzipinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpwd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36m_extract_member\u001b[0;34m(self, member, targetpath, pwd)\u001b[0m\n\u001b[1;32m   1671\u001b[0m         \u001b[0;31m# build the destination pathname, replacing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1672\u001b[0m         \u001b[0;31m# forward slashes to platform specific separators.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1673\u001b[0;31m         \u001b[0marcname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmember\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1675\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maltsep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from nltk import tokenize\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "for article, highlights in cnn_ds['train']:\n",
        "  combination = article + tf.constant(' TL;DR ') + tf.strings.regex_replace(highlights, \"\\n\", \" \")\n",
        "  word_count=len(tokenize.word_tokenize(str(combination.numpy())))\n",
        "  if word_count < 256:\n",
        "    print(combination.numpy())\n",
        "    print(article.numpy())\n",
        "    print(highlights.numpy())\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwHmVEnzxz0u"
      },
      "source": [
        "%%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VhQoiUQGxz0v"
      },
      "outputs": [],
      "source": [
        "import progressbar\n",
        "\n",
        "short_texts = []\n",
        "total = len(cnn_ds['train'])\n",
        "progressbar_update_freq = 1000\n",
        "count = 0\n",
        "\n",
        "widgets = [' [',\n",
        "         progressbar.Timer(format= 'elapsed time: %s'),\n",
        "         '] ',\n",
        "           progressbar.Bar('*'),' (',\n",
        "           progressbar.ETA(), ') ',\n",
        "          ]\n",
        "bar = progressbar.ProgressBar(\n",
        "    maxval=total // progressbar_update_freq + 2,\n",
        "    widgets=widgets).start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vsN56VBqxz0v"
      },
      "outputs": [],
      "source": [
        "for article, highlights in cnn_ds['train']:\n",
        "  combination = article + tf.constant(' TL;DR ') + tf.strings.regex_replace(highlights, \"\\n\", \" \")\n",
        "  word_count = len(tokenize.word_tokenize(str(combination.numpy())))\n",
        "  if word_count < 256:\n",
        "    short_texts.append(combination)\n",
        "  count += 1\n",
        "  if count % progressbar_update_freq == 0:\n",
        "    bar.update(count / progressbar_update_freq)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wwOqmjmxz0v"
      },
      "source": [
        "%%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KCamxdGtxz0v"
      },
      "outputs": [],
      "source": [
        "def save_texts(texts):\n",
        "    np.savez('data/selected_texts.npz', texts)\n",
        "\n",
        "def load_texts():\n",
        "    restored_texts = list()\n",
        "    with np.load('data/selected_texts.npz', allow_pickle=True) as data:\n",
        "      for file in data.files:\n",
        "        restored_texts.extend(data[file].tolist())\n",
        "    return restored_texts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zp8y4IdXxz0v"
      },
      "source": [
        "Save the list of short combinations of articles and summaries (sort of a checkpoint)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "THo4PBOjxz0v",
        "outputId": "2aa210e7-8881-4b9e-fcf9-57046fb0ec94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-a9b94917d011>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msave_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshort_texts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'short_texts' is not defined"
          ]
        }
      ],
      "source": [
        "save_texts(short_texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRHPu1FBxz0v"
      },
      "source": [
        "%%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qNjexdH8xz0v",
        "outputId": "c6528af8-54d2-4868-bfe0-edc9c5c07d31",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘data’: File exists\n",
            "--2023-10-11 12:16:30--  https://github.com/vveloso/ai-in-practice-talk/raw/main/gpt-2/data/selected_texts.npz\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/vveloso/ai-in-practice-talk/main/gpt-2/data/selected_texts.npz [following]\n",
            "--2023-10-11 12:16:30--  https://raw.githubusercontent.com/vveloso/ai-in-practice-talk/main/gpt-2/data/selected_texts.npz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5477897 (5.2M) [application/octet-stream]\n",
            "Saving to: ‘data/selected_texts.npz’\n",
            "\n",
            "data/selected_texts 100%[===================>]   5.22M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2023-10-11 12:16:30 (106 MB/s) - ‘data/selected_texts.npz’ saved [5477897/5477897]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!mkdir data\n",
        "!wget https://github.com/vveloso/ai-in-practice-talk/raw/main/gpt-2/data/selected_texts.npz -O data/selected_texts.npz\n",
        "\n",
        "short_texts = load_texts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSKtLCGHxz0w"
      },
      "source": [
        "%%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "koO2wZWnxz0w"
      },
      "outputs": [],
      "source": [
        "tf_train_ds = tf.data.Dataset.from_tensor_slices(short_texts)\n",
        "processed_ds = tf_train_ds.map(gpt2_preprocessor, tf.data.AUTOTUNE).batch(20).cache().prefetch(tf.data.AUTOTUNE)\n",
        "part_of_ds = processed_ds.take(200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_OZ74Ywxz0w"
      },
      "source": [
        "%%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "1A1SSJ8Fxz0w",
        "outputId": "58b6d4a7-8399-421d-bfc6-67191d279a1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "200/200 [==============================] - 334s 1s/step - loss: 2.5030 - accuracy: 0.4506\n",
            "Epoch 2/2\n",
            "200/200 [==============================] - 248s 1s/step - loss: 2.3592 - accuracy: 0.4704\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7c0ae04bfb20>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "gpt2_lm.include_preprocessing = False\n",
        "\n",
        "num_epochs = 2\n",
        "\n",
        "lr = tf.keras.optimizers.schedules.PolynomialDecay(\n",
        "    5e-5,\n",
        "    decay_steps=part_of_ds.cardinality() * num_epochs,\n",
        "    end_learning_rate=0.0,\n",
        ")\n",
        "\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "gpt2_lm.compile(\n",
        "    optimizer=keras.optimizers.experimental.Adam(lr),\n",
        "    loss=loss,\n",
        "    weighted_metrics=[\"accuracy\"])\n",
        "\n",
        "gpt2_lm.fit(part_of_ds, epochs=num_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qmyfeR-xz0w"
      },
      "source": [
        "%%"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2_lm.generate(\"All flights have been suspended in London's Luton Airport following the breakout of a \\\"significant\\\" fire in the airport's Terminal 2 parking lot, the airport said in a statement on Wednesday. The airport said it would be closed until at least 3 p.m. local time, with passengers advised not to travel to the airport. TL;DR \", max_length=200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPtFizea7iyc",
        "outputId": "e22b532d-f5a8-42fb-9125-edba79e7481a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=string, numpy=b'All flights have been suspended in London\\'s Luton Airport following the breakout of a \"significant\" fire in the airport\\'s Terminal 2 parking lot, the airport said in a statement on Wednesday. The airport said it would be closed until at least 3 p.m. local time, with passengers advised not to travel to the airport. TL;DR The fire at Terminal 2 is \"significant\" airport said. The airport said it will be shut until at least 3 p.m. local time. The blaze occurred in a parking lot in a Terminal 2 parking lot.'>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2_lm.generate(\"The House GOP's two candidates for speaker detailed their plans during a closed-door meeting on Tuesday for avoiding a government shutdown - a key issue for members, and one that sank Kevin McCarthy's speakership. House Majority Leader Steve Scalise and Judiciary Chairman Jim Jordan made their pitches during the Tuesday meeting ahead of a conference vote for speaker on Wednesday, but GOP lawmakers made clear that the conference remains divided, and there's a heavy dose of skepticism among Republicans that they will quickly coalesce around either candidate to be the next speaker. TL;DR \", max_length=200)"
      ],
      "metadata": {
        "outputId": "6fd1184c-2093-40f8-e1c3-bfdbf13b3933",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_d8j3nVIEuX"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=string, numpy=b\"The House GOP's two candidates for speaker detailed their plans during a closed-door meeting on Tuesday for avoiding a government shutdown - a key issue for members, and one that sank Kevin McCarthy's speakership. House Majority Leader Steve Scalise and Judiciary Chairman Jim Jordan made their pitches during the Tuesday meeting ahead of a conference vote for speaker on Wednesday, but GOP lawmakers made clear that the conference remains divided, and there's a heavy dose of skepticism among Republicans that they will quickly coalesce around either candidate to be the next speaker. TL;DR  Speaker Paul Ryan's two contenders for speaker outline their plan. The two candidates are expected to address a closed-door conference vote for Speaker Paul Ryan. The House GOP's two candidates for speaker detailed their plans during closed-door meetings Tuesday.\">"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2_lm.backbone.save_weights(\"finetuned_model.h5\")"
      ],
      "metadata": {
        "id": "WGytoHuEhwmo"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2_lm.backbone.load_weights(\"finetuned_model.h5\")"
      ],
      "metadata": {
        "id": "7UX4YinvzmYR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1LnhcchrUbsu"
      },
      "outputs": [],
      "source": [
        "del gpt2_tokenizer, gpt2_preprocessor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "aNobqZ9UBJJK"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def generate(prompt, max_length):\n",
        "    return gpt2_lm.generate(prompt, max_length)\n",
        "\n",
        "concrete_func = generate.get_concrete_function(tf.TensorSpec([], tf.string), 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "914-vg7ZBOCc"
      },
      "outputs": [],
      "source": [
        "def run_inference(input, generate_tflite):\n",
        "  interp = interpreter.InterpreterWithCustomOps(\n",
        "      model_content=generate_tflite,\n",
        "      custom_op_registerers=tf_text.tflite_registrar.SELECT_TFTEXT_OPS)\n",
        "  interp.get_signature_list()\n",
        "\n",
        "  generator = interp.get_signature_runner('serving_default')\n",
        "  output = generator(prompt=np.array([input]))\n",
        "  print(\"\\nGenerated with TFLite:\\n\", output[\"output_0\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "ZnGHtItBBKEP",
        "outputId": "7596ce1f-15c0-48c5-b6d6-18242dacf4c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras_nlp.models.gpt2.gpt2_causal_lm_preprocessor.GPT2CausalLMPreprocessor object at 0x78abcbd7a680>, because it is not built.\n",
            "WARNING:absl:Found untraced functions such as gpt2_tokenizer_1_layer_call_fn, gpt2_tokenizer_1_layer_call_and_return_conditional_losses, cached_multi_head_attention_layer_call_fn, cached_multi_head_attention_layer_call_and_return_conditional_losses, layer_normalization_layer_call_fn while saving (showing 5 of 314). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated with TFLite:\n",
            " b\"I'm enjoying a great weekend in London with friends and family. I've been looking forward to getting back to my hometown for the first time since the end of the World Cup. The weather is nice, there are no issues, and I'm feeling pretty safe and comfortable. I'll be staying at a lovely hotel with my wife and two young boys, who are both from Manchester City and Chelsea. The weather is good and the sun is setting, so I'm feeling really good! \\xc2\\xa0It\"\n"
          ]
        }
      ],
      "source": [
        "gpt2_lm.jit_compile = False\n",
        "converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func],\n",
        "                                                            gpt2_lm)\n",
        "converter.target_spec.supported_ops = [\n",
        "  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\n",
        "  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\n",
        "]\n",
        "converter.allow_custom_ops = True\n",
        "converter.target_spec.experimental_select_user_tf_ops = [\"UnsortedSegmentJoin\", \"UpperBound\"]\n",
        "converter._experimental_guarantee_all_funcs_one_use = True\n",
        "generate_tflite = converter.convert()\n",
        "run_inference(\"I'm enjoying a\", generate_tflite)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "waKoU0YxH3SE"
      },
      "outputs": [],
      "source": [
        "with open('unquantized_gpt2.tflite', 'wb') as f:\n",
        "  f.write(generate_tflite)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "PYpB2FyA_l3D",
        "outputId": "4f14ba76-5250-4029-8459-1532e9a0b641",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 478M Oct 11 13:18 unquantized_gpt2.tflite\n"
          ]
        }
      ],
      "source": [
        "!ls -lh *.tflite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "kLFzGKIXbPeF",
        "outputId": "bd2777ed-f44c-46c0-b8ba-01bd6ab6310a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras_nlp.models.gpt2.gpt2_causal_lm_preprocessor.GPT2CausalLMPreprocessor object at 0x78abcbd7a680>, because it is not built.\n",
            "WARNING:absl:Found untraced functions such as gpt2_tokenizer_1_layer_call_fn, gpt2_tokenizer_1_layer_call_and_return_conditional_losses, cached_multi_head_attention_layer_call_fn, cached_multi_head_attention_layer_call_and_return_conditional_losses, layer_normalization_layer_call_fn while saving (showing 5 of 314). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated with TFLite:\n",
            " b\"I'm enjoying a lot of things: reading my weekly weekly weekly newsletter, and then following up with some of your favorite\\xc2\\xa0quotes. See my written daily Journalist column, which features weekly written written written written for the Mail Mail.  \\xc2\\xa0You can\\xc2\\xa0quiz my weekly weekly written columns:\\xc2\\xa0quiz:\\xc2\\xa0quiz:\\xc2\\xa0Quiz:\\xc2\\xa0quiz:\\xc2\\xa0quiz:\\xc2\\xa0quiz:\\xc2\\xa0quiz:\\xc2\\xa0quiz:\\n\\xc2\\xa0quiz:\\xc2\\xa0\"\n"
          ]
        }
      ],
      "source": [
        "gpt2_lm.jit_compile = False\n",
        "converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func],\n",
        "                                                            gpt2_lm)\n",
        "converter.target_spec.supported_ops = [\n",
        "  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\n",
        "  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\n",
        "]\n",
        "converter.allow_custom_ops = True\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.target_spec.experimental_select_user_tf_ops = [\"UnsortedSegmentJoin\", \"UpperBound\"]\n",
        "converter._experimental_guarantee_all_funcs_one_use = True\n",
        "quant_generate_tflite = converter.convert()\n",
        "run_inference(\"I'm enjoying a\", quant_generate_tflite)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_inference(\"All flights have been suspended in London's Luton Airport following the breakout of a \\\"significant\\\" fire in the airport's Terminal 2 parking lot, the airport said in a statement on Wednesday. The airport said it would be closed until at least 3 p.m. local time, with passengers advised not to travel to the airport. TL;DR \", quant_generate_tflite)"
      ],
      "metadata": {
        "id": "WpbNScdIx7Rx",
        "outputId": "f2595578-4504-4aa0-f7fc-52ae72965965",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated with TFLite:\n",
            " b'All flights have been suspended in London\\'s Luton Airport following the breakout of a \"significant\" fire in the airport\\'s Terminal 2 parking lot, the airport said in a statement on Wednesday. The airport said it would be closed until at least 3 p.m. local time, with passengers advised not to travel to the airport. TL;DR  Underground fire sparks at Terminal 2 parking lot. The blaze started around 6 p.m., the airport said. The blaze occurred at Terminal 2 parking lot'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_inference(\"All flights have been suspended in London's Luton Airport following the breakout of a \\\"significant\\\" fire in the airport's Terminal 2 parking lot, the airport said in a statement on Wednesday. The airport said it would be closed until at least 3 p.m. local time, with passengers advised not to travel to the airport. TL;DR \", generate_tflite)"
      ],
      "metadata": {
        "id": "_VfnpXjmyO_i",
        "outputId": "9769f7e1-70c0-4e54-fe60-95283f31dbe1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated with TFLite:\n",
            " b'All flights have been suspended in London\\'s Luton Airport following the breakout of a \"significant\" fire in the airport\\'s Terminal 2 parking lot, the airport said in a statement on Wednesday. The airport said it would be closed until at least 3 p.m. local time, with passengers advised not to travel to the airport. TL;DR  Airport shut down in London after a fire in terminal 2 parking lot. A \"significant\" fire broke out in Terminal 2 parking lot at Luton Airport'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "IwWuHksnkeIB"
      },
      "outputs": [],
      "source": [
        "with open('quantized_gpt2.tflite', 'wb') as f:\n",
        "  f.write(quant_generate_tflite)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "zAsRgEonkxoG",
        "outputId": "c7e97a30-0b23-4375-c85b-fee82bcce065",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 124M Oct 11 13:24 quantized_gpt2.tflite\n",
            "-rw-r--r-- 1 root root 478M Oct 11 13:18 unquantized_gpt2.tflite\n"
          ]
        }
      ],
      "source": [
        "!ls -lh *.tflite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "WD5aj3e-tHAh"
      },
      "outputs": [],
      "source": [
        "!mv quantized_gpt2.tflite summarise.tflite"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del quant_generate_tflite, generate_tflite"
      ],
      "metadata": {
        "id": "o9LsFkBa3r07"
      },
      "execution_count": 25,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
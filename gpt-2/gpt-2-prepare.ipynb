{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Using GPT-2 for summarising text\n",
        "\n",
        "This notebook is heavily based on [Google's IO 2023 workshop notebook](https://colab.research.google.com/github/tensorflow/codelabs/blob/main/KerasNLP/io2023_workshop.ipynb), which demonstrates the use of KerasNLP to load a pre-trained GPT-2 model, fine-tune it to a specific text style, and convert it to the TensorFlow Lite format.\n",
        "\n",
        "A lot more details are available in the workshop's notebook.\n",
        "\n",
        "Keep in mind that fine-tuning and using the model requires quite a bit of memory. Fine-tuning requires more than 10GB RAM and some 12GB of GPU RAM. It is recommended to use the TensorFlow Lite model in devices with at least 4G of RAM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dependencies and imports\n",
        "\n",
        "First, we need to load [KerasNLP](https://keras.io/keras_nlp/) into our environment and import all dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kN3h7UYxz0s",
        "outputId": "a1f43531-4b59-4f3b-e592-e6e71ed43bda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q git+https://github.com/keras-team/keras-nlp.git@google-io-2023 tensorflow-text==2.12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SIEWadhKxz0s"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Vasco\\miniconda3\\envs\\tf\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import keras_nlp\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_text as tf_text\n",
        "from tensorflow import keras\n",
        "from tensorflow.lite.python import interpreter\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PkFrgp1xz0t"
      },
      "source": [
        "## Load the pre-trained GPT-2 model\n",
        "\n",
        "We now load the pre-trained GPT-2 model from TensorFlow's respository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gGrymhOxz0t",
        "outputId": "d7270ae9-86c9-4686-f1cb-607e9fc06828"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul), but are not present in its tracked objects:   <tf.Variable 'token_embedding/embeddings:0' shape=(50257, 768) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n"
          ]
        }
      ],
      "source": [
        "gpt2_tokenizer = keras_nlp.models.GPT2Tokenizer.from_preset(\"gpt2_base_en\")\n",
        "gpt2_preprocessor = keras_nlp.models.GPT2CausalLMPreprocessor.from_preset(\n",
        "    \"gpt2_base_en\",\n",
        "    sequence_length=256,\n",
        "    add_end_token=True,\n",
        ")\n",
        "gpt2_lm = keras_nlp.models.GPT2CausalLM.from_preset(\"gpt2_base_en\", preprocessor=gpt2_preprocessor)\n",
        "\n",
        "tl_dr = tf.constant(' TL;DR: ')\n",
        "max_tokens = 512"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTgAfB31xz0t"
      },
      "source": [
        "Once the GPT-2 model is loaded, we can verify that it is working properly by asking it to generate some text from a suitable prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUCSbh8bxz0t"
      },
      "outputs": [],
      "source": [
        "output = gpt2_lm.generate(\"My trip to Yosemite was\", max_length=200)\n",
        "print(\"\\nGPT-2 output:\")\n",
        "print(output.numpy().decode(\"utf-8\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHEYMrJmxz0u"
      },
      "source": [
        "## Fine-tuning GPT-2\n",
        "\n",
        "### Selecting the training data\n",
        "\n",
        "We can try to use the GPT-2 model to summarise texts, but it is also interesting to fine-tune it to a specific text and summary style.\n",
        "\n",
        "The CNN and Daily Mail data set contains news pieces from these organisations together with their summaries.\n",
        "\n",
        "We begin by loading the data set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mSZmf18Fxz0u"
      },
      "outputs": [],
      "source": [
        "cnn_ds = tfds.load('cnn_dailymail', as_supervised=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrCnxSYTxz0u"
      },
      "source": [
        "In their “Language Models are Unsupervised Multitask Learners” paper, Radford et al mention that they used the TL;DR: token to elicit summarising behaviour from GPT-2. We'll follow the same approach when fine-tuning the model by combining each article with its highlights using \"TL;DR:\".\n",
        "\n",
        "We will use only a subset of the CNN and Daily Mail data set: the entries whose combination of the article and the highlights do not exceed 512 tokens as determined by GPT-2's tokeniser.\n",
        "\n",
        "Let's see which entry from the dataset is the first to fulfil this requirement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "ywXIS8ESxz0u",
        "outputId": "e0707857-2fc0-4e00-9aca-dd6c4a9630ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "291\n",
            "b\"By. Associated Press. PUBLISHED:. 14:11 EST, 25 October 2013. |. UPDATED:. 15:36 EST, 25 October 2013. The bishop of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo, Grand Forks and Jamestown to the hepatitis A virus in late September and early October. The state Health Department has issued an advisory of exposure for anyone who attended five churches and took communion. Bishop John Folda (pictured) of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo, Grand Forks and Jamestown to the hepatitis A. State Immunization Program Manager Molly Howell says the risk is low, but officials feel it's important to alert people to the possible exposure. The diocese announced on Monday that Bishop John Folda is taking time off after being diagnosed with hepatitis A. The diocese says he contracted the infection through contaminated food while attending a conference for newly ordained bishops in Italy last month. Symptoms of hepatitis A include fever, tiredness, loss of appetite, nausea and abdominal discomfort. Fargo Catholic Diocese in North Dakota (pictured) is where the bishop is located. TL;DR Bishop John Folda, of North Dakota, is taking time off after being diagnosed. He contracted the infection through contaminated food in Italy. Church members in Fargo, Grand Forks and Jamestown could have been exposed.\"\n",
            "b\"By. Associated Press. PUBLISHED:. 14:11 EST, 25 October 2013. |. UPDATED:. 15:36 EST, 25 October 2013. The bishop of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo, Grand Forks and Jamestown to the hepatitis A virus in late September and early October. The state Health Department has issued an advisory of exposure for anyone who attended five churches and took communion. Bishop John Folda (pictured) of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo, Grand Forks and Jamestown to the hepatitis A. State Immunization Program Manager Molly Howell says the risk is low, but officials feel it's important to alert people to the possible exposure. The diocese announced on Monday that Bishop John Folda is taking time off after being diagnosed with hepatitis A. The diocese says he contracted the infection through contaminated food while attending a conference for newly ordained bishops in Italy last month. Symptoms of hepatitis A include fever, tiredness, loss of appetite, nausea and abdominal discomfort. Fargo Catholic Diocese in North Dakota (pictured) is where the bishop is located.\"\n",
            "b'Bishop John Folda, of North Dakota, is taking time off after being diagnosed.\\nHe contracted the infection through contaminated food in Italy.\\nChurch members in Fargo, Grand Forks and Jamestown could have been exposed.'\n"
          ]
        }
      ],
      "source": [
        "for article, highlights in cnn_ds['train']:\n",
        "  combination = article + tl_dr + tf.strings.regex_replace(highlights, \"\\n\", \" \")\n",
        "  tokens = gpt2_tokenizer.tokenize([str(combination.numpy())])\n",
        "  token_count = tokens.flat_values.shape[0]\n",
        "  if token_count < max_tokens:\n",
        "    print(token_count)\n",
        "    print(combination.numpy())\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwHmVEnzxz0u"
      },
      "source": [
        "Now it is time to create the data subset.\n",
        "\n",
        "This operation may take a very long time, which is why we made a pre-processed subset available in GitHub. You may skip the following preparation code and load it directly using one of the following code sections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "vsN56VBqxz0v"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " [elapsed time: 1 day, 6:50:46] |*************************** | (ETA:  0:12:53) \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 287113 articles of which 39411 were used (had a token count smaller than 512).\n"
          ]
        }
      ],
      "source": [
        "import progressbar\n",
        "\n",
        "short_texts = []\n",
        "total = len(cnn_ds['train'])\n",
        "progressbar_update_freq = 1000\n",
        "count = 0\n",
        "used = 0\n",
        "\n",
        "widgets = [' [',\n",
        "         progressbar.Timer(format= 'elapsed time: %s'),\n",
        "         '] ',\n",
        "           progressbar.Bar('*'),' (',\n",
        "           progressbar.ETA(), ') ',\n",
        "          ]\n",
        "bar = progressbar.ProgressBar(\n",
        "    maxval=total // progressbar_update_freq + 2,\n",
        "    widgets=widgets).start()\n",
        "\n",
        "for article, highlights in cnn_ds['train']:\n",
        "  combination = article + tl_dr + tf.strings.regex_replace(highlights, \"\\n\", \" \")\n",
        "  tokens = gpt2_tokenizer.tokenize([str(combination.numpy())])\n",
        "  token_count = tokens.flat_values.shape[0]\n",
        "  if token_count < max_tokens:\n",
        "    short_texts.append(combination)\n",
        "    used += 1\n",
        "  count += 1\n",
        "  if count % progressbar_update_freq == 0:\n",
        "    bar.update(count / progressbar_update_freq)\n",
        "\n",
        "print(f'Processed {count} articles of which {used} were used (had a token count smaller than {max_tokens}).')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wwOqmjmxz0v"
      },
      "source": [
        "Let's define a pair of helper functions to load and save the data subset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "KCamxdGtxz0v"
      },
      "outputs": [],
      "source": [
        "def save_texts(texts):\n",
        "    np.savez('data/selected_texts.npz', texts)\n",
        "\n",
        "def load_texts():\n",
        "    restored_texts = list()\n",
        "    with np.load('data/selected_texts.npz', allow_pickle=True) as data:\n",
        "      for file in data.files:\n",
        "        restored_texts.extend(data[file].tolist())\n",
        "    return restored_texts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zp8y4IdXxz0v"
      },
      "source": [
        "If you ran the previous data subset preparation, it is always a good idea to save the list of short combinations of articles and summaries (sort of a checkpoint)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "id": "THo4PBOjxz0v",
        "outputId": "2aa210e7-8881-4b9e-fcf9-57046fb0ec94"
      },
      "outputs": [],
      "source": [
        "save_texts(short_texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRHPu1FBxz0v"
      },
      "source": [
        "The pre-prepared data subset can be loaded from GitHub:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNjexdH8xz0v",
        "outputId": "c6528af8-54d2-4868-bfe0-edc9c5c07d31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘data’: File exists\n",
            "--2023-10-11 12:16:30--  https://github.com/vveloso/ai-in-practice-talk/raw/main/gpt-2/data/selected_texts.npz\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/vveloso/ai-in-practice-talk/main/gpt-2/data/selected_texts.npz [following]\n",
            "--2023-10-11 12:16:30--  https://raw.githubusercontent.com/vveloso/ai-in-practice-talk/main/gpt-2/data/selected_texts.npz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5477897 (5.2M) [application/octet-stream]\n",
            "Saving to: ‘data/selected_texts.npz’\n",
            "\n",
            "data/selected_texts 100%[===================>]   5.22M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2023-10-11 12:16:30 (106 MB/s) - ‘data/selected_texts.npz’ saved [5477897/5477897]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!mkdir data\n",
        "!wget https://github.com/vveloso/ai-in-practice-talk/raw/main/gpt-2/data/selected_texts.npz -O data/selected_texts.npz\n",
        "\n",
        "short_texts = load_texts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSKtLCGHxz0w"
      },
      "source": [
        "### Running the fine-tuning training steps\n",
        "\n",
        "We begin by preparing the data set. The data subset selected in the previous steps is now pre-processed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "koO2wZWnxz0w"
      },
      "outputs": [],
      "source": [
        "tf_train_ds = tf.data.Dataset.from_tensor_slices(short_texts)\n",
        "processed_ds = tf_train_ds.map(gpt2_preprocessor, tf.data.AUTOTUNE).batch(20).cache().prefetch(tf.data.AUTOTUNE)\n",
        "part_of_ds = processed_ds.take(2000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_OZ74Ywxz0w"
      },
      "source": [
        "The model is now trained for two epochs.\n",
        "\n",
        "This step may take a while. You may skip this step and load the weights made available at GitHub using one of the following code sections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1A1SSJ8Fxz0w",
        "outputId": "58b6d4a7-8399-421d-bfc6-67191d279a1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "200/200 [==============================] - 334s 1s/step - loss: 2.5030 - accuracy: 0.4506\n",
            "Epoch 2/2\n",
            "200/200 [==============================] - 248s 1s/step - loss: 2.3592 - accuracy: 0.4704\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7c0ae04bfb20>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gpt2_lm.include_preprocessing = False\n",
        "\n",
        "num_epochs = 2\n",
        "\n",
        "lr = tf.keras.optimizers.schedules.PolynomialDecay(\n",
        "    5e-5,\n",
        "    decay_steps=part_of_ds.cardinality() * num_epochs,\n",
        "    end_learning_rate=0.0,\n",
        ")\n",
        "\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "gpt2_lm.compile(\n",
        "    optimizer=keras.optimizers.experimental.Adam(lr),\n",
        "    loss=loss,\n",
        "    weighted_metrics=[\"accuracy\"])\n",
        "\n",
        "gpt2_lm.fit(part_of_ds, epochs=num_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qmyfeR-xz0w"
      },
      "source": [
        "Let's try to ask the fine-tuned model to summarise a short news item."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPtFizea7iyc",
        "outputId": "e22b532d-f5a8-42fb-9125-edba79e7481a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=string, numpy=b'All flights have been suspended in London\\'s Luton Airport following the breakout of a \"significant\" fire in the airport\\'s Terminal 2 parking lot, the airport said in a statement on Wednesday. The airport said it would be closed until at least 3 p.m. local time, with passengers advised not to travel to the airport. TL;DR The fire at Terminal 2 is \"significant\" airport said. The airport said it will be shut until at least 3 p.m. local time. The blaze occurred in a parking lot in a Terminal 2 parking lot.'>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gpt2_lm.generate(\"All flights have been suspended in London's Luton Airport following the breakout of a \\\"significant\\\" fire in the airport's Terminal 2 parking lot, the airport said in a statement on Wednesday. The airport said it would be closed until at least 3 p.m. local time, with passengers advised not to travel to the airport. TL;DR: \", max_length=200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It's always a good idea to save the fine-tuned weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "WGytoHuEhwmo"
      },
      "outputs": [],
      "source": [
        "gpt2_lm.backbone.save_weights(\"data/finetuned_model.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The fine-tuned weights are also available at GitHub, and they can be loaded using the following snippets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!mkdir data\n",
        "!wget https://github.com/vveloso/ai-in-practice-talk/releases/download/20231105/finetuned_model.h5 -O data/finetuned_model.h5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7UX4YinvzmYR"
      },
      "outputs": [],
      "source": [
        "gpt2_lm.backbone.load_weights(\"data/finetuned_model.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's release some memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1LnhcchrUbsu"
      },
      "outputs": [],
      "source": [
        "del gpt2_tokenizer, gpt2_preprocessor, tf_train_ds, processed_ds, part_of_ds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Converting the model to the TensorFlow Lite format\n",
        "\n",
        "Before we can use the model in a mobile application, it needs to be converted to the TensorFlow Lite format.\n",
        "\n",
        "A TensorFlow function is created to simplify using the model with a fixed output of 200 tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "aNobqZ9UBJJK"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def generate(prompt, max_length):\n",
        "    return gpt2_lm.generate(prompt, max_length)\n",
        "\n",
        "concrete_func = generate.get_concrete_function(tf.TensorSpec([], tf.string), 200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We new define a helper function to test the TensorFlow Lite models once they are created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "914-vg7ZBOCc"
      },
      "outputs": [],
      "source": [
        "def run_inference(input, generate_tflite):\n",
        "  interp = interpreter.InterpreterWithCustomOps(\n",
        "      model_content=generate_tflite,\n",
        "      custom_op_registerers=tf_text.tflite_registrar.SELECT_TFTEXT_OPS)\n",
        "  interp.get_signature_list()\n",
        "\n",
        "  generator = interp.get_signature_runner('serving_default')\n",
        "  output = generator(prompt=np.array([input]))\n",
        "  print(\"\\nGenerated with TFLite:\\n\", output[\"output_0\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Conversion of the GPT-2 model to TensorFlow Lite requires the use of TensorFlow operands not normally included; they need to be specified during the conversion. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnGHtItBBKEP",
        "outputId": "7596ce1f-15c0-48c5-b6d6-18242dacf4c5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras_nlp.models.gpt2.gpt2_causal_lm_preprocessor.GPT2CausalLMPreprocessor object at 0x78abcbd7a680>, because it is not built.\n",
            "WARNING:absl:Found untraced functions such as gpt2_tokenizer_1_layer_call_fn, gpt2_tokenizer_1_layer_call_and_return_conditional_losses, cached_multi_head_attention_layer_call_fn, cached_multi_head_attention_layer_call_and_return_conditional_losses, layer_normalization_layer_call_fn while saving (showing 5 of 314). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Generated with TFLite:\n",
            " b\"I'm enjoying a great weekend in London with friends and family. I've been looking forward to getting back to my hometown for the first time since the end of the World Cup. The weather is nice, there are no issues, and I'm feeling pretty safe and comfortable. I'll be staying at a lovely hotel with my wife and two young boys, who are both from Manchester City and Chelsea. The weather is good and the sun is setting, so I'm feeling really good! \\xc2\\xa0It\"\n"
          ]
        }
      ],
      "source": [
        "gpt2_lm.jit_compile = False\n",
        "converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func],\n",
        "                                                            gpt2_lm)\n",
        "converter.target_spec.supported_ops = [\n",
        "  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\n",
        "  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\n",
        "]\n",
        "converter.allow_custom_ops = True\n",
        "converter.target_spec.experimental_select_user_tf_ops = [\"UnsortedSegmentJoin\", \"UpperBound\"]\n",
        "converter._experimental_guarantee_all_funcs_one_use = True\n",
        "generate_tflite = converter.convert()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now save the converted model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "waKoU0YxH3SE"
      },
      "outputs": [],
      "source": [
        "with open('unquantized_gpt2.tflite', 'wb') as f:\n",
        "  f.write(generate_tflite)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's see how big it is."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYpB2FyA_l3D",
        "outputId": "4f14ba76-5250-4029-8459-1532e9a0b641"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-rw-r--r-- 1 root root 478M Oct 11 13:18 unquantized_gpt2.tflite\n"
          ]
        }
      ],
      "source": [
        "!ls -lh *.tflite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The converted TensorFlow Lite model is still very large. We can quantise the TensorFlow Lite model during conversion to reduce its size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLFzGKIXbPeF",
        "outputId": "bd2777ed-f44c-46c0-b8ba-01bd6ab6310a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras_nlp.models.gpt2.gpt2_causal_lm_preprocessor.GPT2CausalLMPreprocessor object at 0x78abcbd7a680>, because it is not built.\n",
            "WARNING:absl:Found untraced functions such as gpt2_tokenizer_1_layer_call_fn, gpt2_tokenizer_1_layer_call_and_return_conditional_losses, cached_multi_head_attention_layer_call_fn, cached_multi_head_attention_layer_call_and_return_conditional_losses, layer_normalization_layer_call_fn while saving (showing 5 of 314). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Generated with TFLite:\n",
            " b\"I'm enjoying a lot of things: reading my weekly weekly weekly newsletter, and then following up with some of your favorite\\xc2\\xa0quotes. See my written daily Journalist column, which features weekly written written written written for the Mail Mail.  \\xc2\\xa0You can\\xc2\\xa0quiz my weekly weekly written columns:\\xc2\\xa0quiz:\\xc2\\xa0quiz:\\xc2\\xa0Quiz:\\xc2\\xa0quiz:\\xc2\\xa0quiz:\\xc2\\xa0quiz:\\xc2\\xa0quiz:\\xc2\\xa0quiz:\\n\\xc2\\xa0quiz:\\xc2\\xa0\"\n"
          ]
        }
      ],
      "source": [
        "gpt2_lm.jit_compile = False\n",
        "converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func],\n",
        "                                                            gpt2_lm)\n",
        "converter.target_spec.supported_ops = [\n",
        "  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\n",
        "  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\n",
        "]\n",
        "converter.allow_custom_ops = True\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.target_spec.experimental_select_user_tf_ops = [\"UnsortedSegmentJoin\", \"UpperBound\"]\n",
        "converter._experimental_guarantee_all_funcs_one_use = True\n",
        "quant_generate_tflite = converter.convert()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now save this new version and check its size. You'll see that it is considerably smaller."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('quantized_gpt2.tflite', 'wb') as f:\n",
        "  f.write(quant_generate_tflite)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-rw-r--r-- 1 root root 124M Oct 11 13:24 quantized_gpt2.tflite\n",
            "-rw-r--r-- 1 root root 478M Oct 11 13:18 unquantized_gpt2.tflite\n"
          ]
        }
      ],
      "source": [
        "!ls -lh *.tflite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's try both models out. First, the quantised version and then the non-quantised version. Both should generate acceptable summaries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpbNScdIx7Rx",
        "outputId": "f2595578-4504-4aa0-f7fc-52ae72965965"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Generated with TFLite:\n",
            " b'All flights have been suspended in London\\'s Luton Airport following the breakout of a \"significant\" fire in the airport\\'s Terminal 2 parking lot, the airport said in a statement on Wednesday. The airport said it would be closed until at least 3 p.m. local time, with passengers advised not to travel to the airport. TL;DR  Underground fire sparks at Terminal 2 parking lot. The blaze started around 6 p.m., the airport said. The blaze occurred at Terminal 2 parking lot'\n"
          ]
        }
      ],
      "source": [
        "run_inference(\"All flights have been suspended in London's Luton Airport following the breakout of a \\\"significant\\\" fire in the airport's Terminal 2 parking lot, the airport said in a statement on Wednesday. The airport said it would be closed until at least 3 p.m. local time, with passengers advised not to travel to the airport. TL;DR \", quant_generate_tflite)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VfnpXjmyO_i",
        "outputId": "9769f7e1-70c0-4e54-fe60-95283f31dbe1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Generated with TFLite:\n",
            " b'All flights have been suspended in London\\'s Luton Airport following the breakout of a \"significant\" fire in the airport\\'s Terminal 2 parking lot, the airport said in a statement on Wednesday. The airport said it would be closed until at least 3 p.m. local time, with passengers advised not to travel to the airport. TL;DR  Airport shut down in London after a fire in terminal 2 parking lot. A \"significant\" fire broke out in Terminal 2 parking lot at Luton Airport'\n"
          ]
        }
      ],
      "source": [
        "run_inference(\"All flights have been suspended in London's Luton Airport following the breakout of a \\\"significant\\\" fire in the airport's Terminal 2 parking lot, the airport said in a statement on Wednesday. The airport said it would be closed until at least 3 p.m. local time, with passengers advised not to travel to the airport. TL;DR \", generate_tflite)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
